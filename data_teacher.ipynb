{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/2018_data_clean/"
      ],
      "metadata": {
        "id": "wcjtJvN5-MDf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ],
      "metadata": {
        "id": "zFcFjjqo-N_f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_COLAB:\n",
        "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "    !wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "    !tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "    !mv spark-3.3.2-bin-hadoop3 spark\n",
        "    !pip install -q findspark\n",
        "    import os\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"/content/spark\""
      ],
      "metadata": {
        "id": "faCEidnS-OHU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "spark_url = 'local'\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(spark_url)\\\n",
        "        .appName('Spark SQL')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "g8kRtnw8-OLt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# 1. แตกไฟล์ ZIP\n",
        "input_path = \"/content/2023.zip\"\n",
        "output_folder = \"/content/2023_extracted\"\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(input_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_folder)\n",
        "\n",
        "print(f\"Files extracted to: {output_folder}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utibP7Pz-hdP",
        "outputId": "288fc5fa-f86f-44a9-bb8f-81c3995f7c03"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content/2023_extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LAMzXTU-FZf",
        "outputId": "e50c1e68-8994-4a6a-d9f2-dbbd40e44a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 2890\n",
            "Data successfully written to /content/2023_coredata\n"
          ]
        }
      ],
      "source": [
        "# Paths\n",
        "folder_path = r\"/content/2023_extracted\"\n",
        "output_csv = r\"/content/2023_coredata\"\n",
        "\n",
        "# Function to process each file\n",
        "def process_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
        "        try:\n",
        "            data = json.load(json_file)\n",
        "\n",
        "            # Ensure the JSON is a dictionary\n",
        "            if not isinstance(data, dict):\n",
        "                print(f\"Unexpected structure in file: {file_path}\")\n",
        "                return [Row(filename=os.path.basename(file_path), status=\"Invalid JSON structure\")]\n",
        "\n",
        "            # Extract coredata, affiliations, and subject-areas\n",
        "            coredata = data.get('abstracts-retrieval-response', {}).get('coredata', {})\n",
        "            affiliations = data.get('abstracts-retrieval-response', {}).get('affiliation', [])\n",
        "            subject_areas = data.get('abstracts-retrieval-response', {}).get('subject-areas', {}).get('subject-area', [])\n",
        "\n",
        "            # Handle cases where affiliation is not a list\n",
        "            if isinstance(affiliations, dict):  # If it's a single dictionary, convert it to a list\n",
        "                affiliations = [affiliations]\n",
        "            elif not isinstance(affiliations, list):  # Ensure it's a list\n",
        "                affiliations = []\n",
        "\n",
        "            # Ensure subject-areas is a list\n",
        "            if not isinstance(subject_areas, list):\n",
        "                subject_areas = []\n",
        "\n",
        "            rows = []\n",
        "            for affiliation in affiliations:\n",
        "                # Ensure the affiliation is a dictionary\n",
        "                if not isinstance(affiliation, dict):\n",
        "                    continue\n",
        "\n",
        "                for subject in subject_areas:\n",
        "                    # Ensure the subject is a dictionary\n",
        "                    if not isinstance(subject, dict):\n",
        "                        continue\n",
        "\n",
        "                    row = Row(\n",
        "                        filename=os.path.basename(file_path),\n",
        "                        status=\"Processed\",\n",
        "                        srctype=coredata.get('srctype', None),\n",
        "                        eid=coredata.get('eid', None),\n",
        "                        prism_coverDate=coredata.get('prism:coverDate', None),\n",
        "                        prism_aggregationType=coredata.get('prism:aggregationType', None),\n",
        "                        subtypeDescription=coredata.get('subtypeDescription', None),\n",
        "                        affiliation_city=affiliation.get('affiliation-city', None),\n",
        "                        affiliation_id=affiliation.get('@id', None),\n",
        "                        affiliation_name=affiliation.get('affilname', None),\n",
        "                        affiliation_country=affiliation.get('affiliation-country', None),\n",
        "                        subject_area_name=subject.get('$', None),\n",
        "                        subject_area_code=subject.get('@code', None),\n",
        "                        subject_area_abbrev=subject.get('@abbrev', None),\n",
        "                        publicationName=coredata.get('prism:publicationName', None),\n",
        "                        source_id=coredata.get('source-id', None),\n",
        "                        citedby_count=coredata.get('citedby-count', None),\n",
        "                        subtype=coredata.get('subtype', None),\n",
        "                        dc_title=coredata.get('dc:title', None),\n",
        "                        prism_doi=coredata.get('prism:doi', None),\n",
        "                        dc_identifier=coredata.get('dc:identifier', None),\n",
        "                        dc_publisher=coredata.get('dc:publisher', None)\n",
        "                    )\n",
        "                    rows.append(row)\n",
        "\n",
        "            if not rows:  # No valid rows, return a default\n",
        "                rows.append(Row(\n",
        "                    filename=os.path.basename(file_path),\n",
        "                    status=\"No valid data\",\n",
        "                    srctype=None, eid=None, prism_coverDate=None,\n",
        "                    prism_aggregationType=None, subtypeDescription=None,\n",
        "                    affiliation_city=None, affiliation_id=None, affiliation_name=None,\n",
        "                    affiliation_country=None,\n",
        "                    subject_area_name=None, subject_area_code=None, subject_area_abbrev=None,\n",
        "                    publicationName=None, source_id=None, citedby_count=None,\n",
        "                    subtype=None, dc_title=None, prism_doi=None,\n",
        "                    dc_identifier=None, dc_publisher=None\n",
        "                ))\n",
        "\n",
        "            return rows\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
        "            return [Row(\n",
        "                filename=os.path.basename(file_path),\n",
        "                status=\"JSON Decode Error\",\n",
        "                srctype=None, eid=None, prism_coverDate=None,\n",
        "                prism_aggregationType=None, subtypeDescription=None,\n",
        "                affiliation_city=None, affiliation_id=None, affiliation_name=None,\n",
        "                affiliation_country=None,\n",
        "                subject_area_name=None, subject_area_code=None, subject_area_abbrev=None,\n",
        "                publicationName=None, source_id=None, citedby_count=None,\n",
        "                subtype=None, dc_title=None, prism_doi=None,\n",
        "                dc_identifier=None, dc_publisher=None\n",
        "            )]\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error in file {file_path}: {e}\")\n",
        "            return [Row(\n",
        "                filename=os.path.basename(file_path),\n",
        "                status=\"Unexpected Error\",\n",
        "                srctype=None, eid=None, prism_coverDate=None,\n",
        "                prism_aggregationType=None, subtypeDescription=None,\n",
        "                affiliation_city=None, affiliation_id=None, affiliation_name=None,\n",
        "                affiliation_country=None,\n",
        "                subject_area_name=None, subject_area_code=None, subject_area_abbrev=None,\n",
        "                publicationName=None, source_id=None, citedby_count=None,\n",
        "                subtype=None, dc_title=None, prism_doi=None,\n",
        "                dc_identifier=None, dc_publisher=None\n",
        "            )]\n",
        "\n",
        "# Collect all files in the folder\n",
        "files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "print(f\"Total files: {len(files)}\")\n",
        "\n",
        "# Process files\n",
        "rdd = spark.sparkContext.parallelize(files).flatMap(process_file)\n",
        "\n",
        "# Define schema dynamically\n",
        "columns = [\n",
        "    \"filename\", \"status\", \"srctype\", \"eid\", \"prism_coverDate\", \"prism_aggregationType\", \"subtypeDescription\",\n",
        "    \"affiliation_city\", \"affiliation_id\", \"affiliation_name\", \"affiliation_country\",\n",
        "    \"subject_area_name\", \"subject_area_code\", \"subject_area_abbrev\",\n",
        "    \"publicationName\", \"source_id\", \"citedby_count\", \"subtype\", \"dc_title\", \"prism_doi\", \"dc_identifier\", \"dc_publisher\"\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(rdd)\n",
        "\n",
        "# Show a sample of the DataFrame\n",
        "df = df.orderBy(\"filename\")\n",
        "\n",
        "# Write to CSV\n",
        "df.write.csv(output_csv, header=True, mode=\"overwrite\")\n",
        "\n",
        "print(f\"Data successfully written to {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# สร้าง SparkSession\n",
        "spark = SparkSession.builder.appName(\"FilterDC_Title\").getOrCreate()\n",
        "\n",
        "# โหลดข้อมูลจากไฟล์ CSV\n",
        "file_path = \"/content/2023_coredata/part-00000-64fc6d7b-b3a3-4502-ae65-2129dfc22f83-c000.csv\"  # เปลี่ยนตาม path ของไฟล์จริง\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# ลบคอลัมน์ที่ไม่ต้องการ\n",
        "columns_to_drop = [\"affiliation_city\", \"affiliation_id\", \"affiliation_name\"]\n",
        "df = df.drop(*columns_to_drop)\n",
        "\n",
        "# เลือก affiliation_country ที่ต่างกันในแต่ละ dc_title\n",
        "result_df = df.dropDuplicates([\"dc_title\", \"affiliation_country\"])\n",
        "\n",
        "result_df = result_df.orderBy(\"filename\")\n",
        "\n",
        "# จัดเรียงคอลัมน์ใหม่\n",
        "result_df = result_df.select(\"filename\", \"dc_title\", \"affiliation_country\", \"srctype\", \"eid\", \"prism_coverDate\", \"prism_aggregationType\", \"subtypeDescription\",\n",
        "    \"subject_area_name\", \"subject_area_code\", \"subject_area_abbrev\",\n",
        "    \"publicationName\", \"source_id\", \"citedby_count\", \"subtype\", \"prism_doi\", \"dc_identifier\", \"dc_publisher\")\n",
        "\n",
        "# หากต้องการบันทึกผลลัพธ์ลงไฟล์\n",
        "output_path = \"/content/2023_data_clean\"\n",
        "result_df.write.csv(output_path, header=True)\n"
      ],
      "metadata": {
        "id": "-Tol_FXIAmvu"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}